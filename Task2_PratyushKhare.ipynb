{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "wy5sWwUhqVNz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = \"TASK2_dataset.csv\"\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_keywords = list(set(df['keyword_1'].tolist() + df['keyword_2'].tolist() + df['keyword_3'].tolist()))\n",
        "\n",
        "keyword_sets = [[row['keyword_1'], row['keyword_2'], row['keyword_3']] for _, row in df.iterrows()]\n",
        "\n",
        "def create_keyword_vector(keywords, reference_keywords):\n",
        "    vector = np.zeros(len(reference_keywords))\n",
        "    for keyword in keywords:\n",
        "        if keyword in reference_keywords:\n",
        "            vector[reference_keywords.index(keyword)] += 1\n",
        "    return vector\n",
        "\n",
        "keyword_vectors = np.array([create_keyword_vector(keywords, dataset_keywords) for keywords in keyword_sets])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3wW897p5rOFa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(data, num_components=2):\n",
        "    mean_adjusted = data - np.mean(data, axis=0)\n",
        "    covariance_matrix = np.cov(mean_adjusted, rowvar=False)\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    principal_components = eigenvectors[:, sorted_indices[:num_components]]\n",
        "    return np.dot(mean_adjusted, principal_components)\n",
        "\n",
        "dimensionality_reduced_vectors = apply_pca(keyword_vectors, num_components=2)\n",
        "scaled_vectors = dimensionality_reduced_vectors / 3\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "8TuHxGmsrRo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_random_centroids(data, k):\n",
        "    return data[np.random.choice(len(data), k, replace=False)]\n",
        "\n",
        "def assign_to_clusters(data, centroids):\n",
        "    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
        "    return np.argmin(distances, axis=1)\n",
        "\n",
        "def compute_new_centroids(data, labels, k):\n",
        "    return np.array([data[labels == i].mean(axis=0) if len(data[labels == i]) > 0 else data[np.random.choice(len(data))] for i in range(k)])\n",
        "\n",
        "def k_means_clustering(data, k, max_iterations=100, tolerance=1e-4):\n",
        "    centroids = initialize_random_centroids(data, k)\n",
        "    for _ in range(max_iterations):\n",
        "        old_centroids = centroids.copy()\n",
        "        cluster_labels = assign_to_clusters(data, centroids)\n",
        "        centroids = compute_new_centroids(data, cluster_labels, k)\n",
        "        if np.linalg.norm(centroids - old_centroids) < tolerance:\n",
        "            break\n",
        "    return cluster_labels, centroids"
      ],
      "metadata": {
        "id": "wWLxC5myr3vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sse = []\n",
        "k_values = range(2, 10)\n",
        "for k in k_values:\n",
        "    labels, centroids = k_means_clustering(scaled_vectors, k)\n",
        "    sse.append(np.sum((scaled_vectors - centroids[labels]) ** 2))\n"
      ],
      "metadata": {
        "id": "fvLlzKJSr9VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, sse, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Sum of Squared Errors (SSE)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tIzRrGygsA-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 5\n",
        "cluster_labels, final_centroids = k_means_clustering(scaled_vectors, num_clusters)\n",
        "df['Cluster'] = cluster_labels"
      ],
      "metadata": {
        "id": "4bkJk1ifsA8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "for i in range(num_clusters):\n",
        "    plt.scatter(scaled_vectors[cluster_labels == i, 0], scaled_vectors[cluster_labels == i, 1],\n",
        "                color=colors[i], label=f'Cluster {i}')\n",
        "plt.scatter(final_centroids[:, 0], final_centroids[:, 1], color='black', marker='x', s=200, label='Centroids')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Cluster Visualization')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Afyq_BZhsH6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_silhouette_score(data, labels):\n",
        "    scores = []\n",
        "    for i in range(len(data)):\n",
        "        same_cluster = data[labels == labels[i]]\n",
        "        intra_distance = np.mean(np.linalg.norm(same_cluster - data[i], axis=1))\n",
        "        inter_distances = [np.mean(np.linalg.norm(data[labels == j] - data[i], axis=1)) for j in set(labels) if j != labels[i]]\n",
        "        nearest_cluster_distance = np.min(inter_distances) if inter_distances else 0\n",
        "        scores.append((nearest_cluster_distance - intra_distance) / max(intra_distance, nearest_cluster_distance))\n",
        "    return np.mean(scores)\n",
        "\n",
        "silhouette = compute_silhouette_score(scaled_vectors, cluster_labels)\n",
        "print(\"Silhouette Score:\", silhouette)"
      ],
      "metadata": {
        "id": "nFnejkZisH2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_genre_distribution = df.groupby('Cluster')['genre'].value_counts(normalize=True) * 100\n",
        "print(\"\\nCluster Genre Distribution:\\n\", cluster_genre_distribution)\n",
        "\n",
        "def apply_pca_to_single_vector(data, reference_data, num_components=2):\n",
        "    mean_adjusted = data - np.mean(reference_data, axis=0)\n",
        "    covariance_matrix = np.cov(reference_data.T)\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    principal_components = eigenvectors[:, sorted_indices[:num_components]]\n",
        "    return np.dot(mean_adjusted, principal_components)"
      ],
      "metadata": {
        "id": "AMtljVjksHxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_genre_for_new_song(keywords, dataset_keywords, centroids, df):\n",
        "    keyword_vector = create_keyword_vector(keywords, dataset_keywords).reshape(1, -1)\n",
        "    reduced_vector = apply_pca_to_single_vector(keyword_vector, keyword_vectors, num_components=2) / 3\n",
        "    distances = np.linalg.norm(centroids - reduced_vector, axis=1)\n",
        "    closest_cluster = np.argmin(distances)\n",
        "    cluster_genres = df[df['Cluster'] == closest_cluster]['genre']\n",
        "    return cluster_genres.mode()[0] if not cluster_genres.empty else \"Unknown\"\n",
        "\n",
        "new_songs = [\n",
        "    ['piano', 'calm', 'slow'],\n",
        "    ['guitar', 'emotional', 'distorted'],\n",
        "    ['synth', 'mellow', 'distorted']\n",
        "]\n",
        "\n",
        "predicted_genres = {tuple(song): predict_genre_for_new_song(song, dataset_keywords, final_centroids, df) for song in new_songs}\n",
        "\n",
        "print(\"\\nNew Song Genre Predictions:\")\n",
        "for song, genre in predicted_genres.items():\n",
        "    print(f\"Keywords: {song} -> Assigned Genre: {genre}\")\n"
      ],
      "metadata": {
        "id": "y3TKi6fAsA21"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}